\documentclass[11pt, letterpaper, notitlepage]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage[margin=.9in]{geometry}
\usepackage[onehalfspacing]{setspace}
\usepackage{dsfont}
\usepackage[round]{natbib}
\usepackage{hyperref}
\usepackage{centernot}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{caption}
%\captionsetup[table]{labelformat=empty}

\setlist[description]{font=\normalfont}

\definecolor{blue}{RGB}{26, 110, 178}
\definecolor{orange}{RGB}{250, 120, 10}
\definecolor{Red}{RGB}{162, 27, 27}
\definecolor{mygray}{gray}{0.6}

\hypersetup{
        colorlinks=true, %false
        linkcolor=orange,
        citecolor=blue,
        urlcolor=Red,     
        pdfborder={0 0 0},
}

\newtheorem{theorem}{Theorem}
\newtheorem{mydef}{Definition}
\newtheorem{myprop}{Proposition}
\newtheorem{myass}{Assumption}
\newcommand{\pro}{\noindent \textsc{Proof: }}

\title{User's Guide for the Bayes GMM estimator}
\author{}
\date{\today}

\begin{document}

\maketitle

\setlength{\parskip}{9pt}
\setlength{\parindent}{0pt}

This is a guide for the C++ program that implements the Bayesian GMM estimator of \cite{GGR_2017}. The program is \textbf{heavily} relies on Gallant's \href{https://www.aronaldg.org/webfiles/mle/}{MLE package}.   

\section*{Directory Structure}

\begin{description}

\item[\textbf{base\_model}:] this folder contains the heart of the MCMC estimator. In principle, this part should be independent from the specific project.
\begin{description}
\item[\textbf{initialize:}] reads the InputParamFile and defines the \textbf{specification class}
\begin{itemize}
\item[-] \emph{source files:} main.cpp, initialize.cpp 
\item[-] \emph{header files:} initialize.h 
\end{itemize}
\item[\textbf{estimator:}] elements of the mcmc sampler/optimizer, generates an \textbf{mcmc class}
\begin{itemize}
\item[-] \emph{source files:} asymptotics.cpp, mcmc\_class.cpp, proposal.cpp 
\item[-] \emph{header files:} estimator\_base.h, estimator.h 
\end{itemize}
\item[\textbf{libscl:}] slightly altered version of Gallant's \href{https://www.aronaldg.org/webfiles/libscl}{statistical library} (including the \textbf{gmm class}) 
\end{description}
\item[\textbf{***.example:}] these foders belong to separate projects (indentified by the *** prefix).\footnote{In addition to the subfolders detailed below, they contain (1) the makefile that generates the executable (called \texttt{bayes\_gmm}), (2) the InputParamFile detailing the specifics of the estimator (3) a python script generating summary statistics and plots from the result files.} Each project directory must contain three subfolders
\begin{description}
\item[\textbf{usermodel:}] program codes that define the \textbf{usermodel class} (see XXX)
\begin{itemize}
\item[-] \emph{source files:} usermodel.cpp, moments.cpp, model.cpp, default\_params.cpp 
\item[-] \emph{header files:} usermodel.h, moments.h, model.h, default\_params.h
\end{itemize}
\item[\textbf{data}:] contains the data (in file \texttt{data.dat}) and the \texttt{initial\_particle.dat} file containing an intial draw of particles for the conditional particle filter. 
%(\textbf{TODO:})There is also a .cpp file that by using components of the usermodel class simulates these two files.   
\item[\textbf{result\_files}:] a plethora of .dat files generated by the estimator for diagnoses and further analyses
\end{description}
\end{description}

\pagebreak

\section{Usage}

Download \texttt{bayes\_gmm.tar} from \textcolor{red}{XXX}. On a Unix machine use \texttt{tar -xf bayes\_gmm.tar} to expand the tar archive into a directory that will be named \texttt{bayes\_gmm}.

\subsection{Sequential version}

Change directory to \texttt{bayes\_gmm/***\_example/},\footnote{Currently \texttt{***} stands for either \texttt{sv} (stochastic volatility model defined in Section 5.1. of \cite{GGR_2017}) or \texttt{sdf} (stochastic discount factor with latent state)} and type \texttt{make} and  the \texttt{bayes\_gmm} executable will be built and ready to run.

This folder also contains a file called \texttt{control.dat}. This file contains the names of the InputParam Files (see later) and the prefix for the generated output files (denoted by \texttt{***}). Here is an example of a one line \texttt{control.dat} file:

\texttt{test.param.000 test}

The input paramfile is named \texttt{test.param.000} (must be in the same folder) and all output files such as \texttt{detail.dat},
\texttt{pi.000.dat}, etc. are named \texttt{test.detail.dat}, \texttt{test.pi.000.dat}, etc. in the corresponding \texttt{result\_files} folder. The key result files are:
\begin{itemize}
\item \texttt{detail.dat}: Voluminous detailed output from the run.
\item \texttt{summary.dat}: This file summarizes the output giving mean, mode, and standard errors
\item \texttt{theta.000.dat}: contains
the MCMC chain for $\theta$ 
\item \texttt{pi.000.dat}: Let $\ell(\theta) = \exp(-n s_n(\theta))$, and let $p(\theta)$ denote the prior. This file contains three items corresponding to the MCMC chain for $\theta$ :
(1) $\log \ell(\theta) + \log p(\theta)$, (2) $\log \ell(\theta)$, (3) $\log p(\theta)$.
\item \texttt{reject.000.dat}: this contains a matrix whose first column contains the rejection rate for each parameter followed by the overall rejection rate. ADD: other columns
\item \texttt{paramfile.fit}: A copy of the InputParamFile with the parameter start values replaced by the mode and scaling variables recomputed so that \texttt{proposal\_scale\_factor} is 1.0. All else is the same as the InputParamFile.
\item \texttt{theta\_mode}, \texttt{theta\_mode}, \texttt{V\_hat\_hess}, etc.: Statistics from the run in the form expected for reading with member \texttt{vecread} of class \texttt{scl::realmat} in library \texttt{libscl}.
\end{itemize}

\subsection{Parallel version with OpenMPI}

\pagebreak 

\section{The InputParam File}

The InputParam File (typically named as \texttt{***.param.000}) contains several blocks of control information. The structure is the same as the paramfiles for Gallant's EMM and MLE packages. The followings are mostly extracts from his MLE guide.

\subsection{PARAMFILE HISTORY}

This part is optional. It is written automatically by the program to the output paramfile called \texttt{***.paramfile.fit} (located in the \texttt{result\_files} folder) at the end of every run. It consists of seven lines that begin with \# that should be left alone. After these seven lines, the user can add additional lines\footnote{Specifically, the three paramfiles (\texttt{***.paramfile.fit}, \texttt{***.paramfile.alt}, \texttt{***.paramfile.end}) are being written by the function \texttt{initialize::specification\_class::write\_params()} which is defined in \texttt{initialize.cpp}.} (e.g. describing the model) that begin with a \# and these will get copied from the input parmfile to the output parmfile.  

\textbf{Example:}

\begin{verbatim}
PARAMFILE HISTORY (optional)
#
# This paramfile was written by bayes_gmm 1.0 using the following line from
# control.dat, which was read as char*, char*
# ---------------------------------------------------------------------------
#      test.param.000                test
# ---------------------------------------------------------------------------
#
\end{verbatim}

\subsection{ESTIMATION DESCRIPTION}

The $13$ lines specify values for parameters of the estimator \textbf{(order matters!)}. Each line starts with $12$ characters including the parameter value, then the following pieces of information separated by comma: (1) description, (2) variable name, (3) variable type. 


\texttt{proposaltype:} Standard is the \emph{group move} proposal which defaults to a single move proposal when the optional InputParamFile block PROPOSAL GROUPING is missing. When the PROPOSAL GROUPING block is missing, the \texttt{proposaltype=0} proposal randomly selects an element of $\theta$ to move and the draws from a normal; i.e. a move-one-at-a-time random walk. When PROPOSAL GROUPING block is present, the proposal randomly selects one of the groups defined therein to move and draws from a user specified multivariate normal. 

\texttt{ask\_print:} If the value is $1$, then voluminous debugging information is written to file \texttt{***.detail.dat} in the \texttt{result\_files.dat} subdirectory. Setting it ot $0$ suppresses printing.

\texttt{iseed:} Seed for the MCMC chain.

\texttt{num\_mcmc\_draws:} The MCMC chain is broken up into pieces and written to files \texttt{theta.000.dat}, \texttt{theta.001.dat}, etc. This variable determines the number of draws per file.

\texttt{num\_mcmc\_files:}: Determines how many files in addition to \texttt{theta.000.dat} are generated. The total length of the MCMC chain is \texttt{R= num\_mcmc\_draws*(num\_mcmc\_files + 1)}. Many other files are produced to describe the chain such as \texttt{reject.000.dat}, \texttt{pi.000.dat}, \texttt{stats.000.dat} as well as summary files, files containing variance matrices, etc. 

\texttt{proposal\_scale\_factor:} Rescales the proposal standard deviations that are set in the PROPOSAL SCALING
block without changing relative values.

\texttt{temperature:} \textcolor{red}{For Bayesian inference it is essential that temperature = 1!} Otherwise, this variable controls the peakedness of the objective function (e.g. likelihood). Putting \texttt{temperature = 2} is like doubling the number of observations from which the likelihood was computed, which makes the objective function more peaked. Putting \texttt{temperature = 0.5} would be like halving them. 

\texttt{no\_sandwich:} Computing sandwich standard errors is costly and often unnecessary, setting this variable to 1 will stop them from being computed. Even for an estimator that does require the computation of sandwich standard errors, one should set \texttt{no\_sandwich = 1} during the early hill climbing phase of the chain. When the objective function has reached its plateau and the stationary portion of the chain has been reached, \texttt{no\_sandwich} can be set to 0. 

\texttt{lag\_hac}: The number of lags to be used to compute the HAC information matrix in the middle of the sandwich variance estimator. Set \texttt{lag\_hac=0} if the scores are uncorrelated, in which case the estimator is heteroskedastic consistent.

\texttt{thin:} The program writes the MCMC chains to files of length \texttt{num\_mcmc\_draws} as explained above. If \texttt{thin=1}, every element of the chain is written. If \texttt{thin=2}, every other element is written and the length of an output files becomes \texttt{num\_mcmc\_draws/2}. Similarly for higher values of \texttt{thin}. Thin greater than one reduces memory requirements because values not written are not stored anywhere. One consequence of this is that statistics such as the Hessian are computed only from the elements of the MCMC chain that are written, not from all that are generated. The exceptions are that the mode and the rejection count are computed \textbf{from all elements} that were generated.

\texttt{draw\_from\_prior:} When the prior is proper, it is useful to be able to draw from the prior for at least two purposes. The first is to be able to compare the prior and posterior distribution of estimates of parameters and functionals. The other is as an intermediate step in computing posterior probabilities for model selection. The essential information for model selection is in the output files named \texttt{pi.000.dat}, \texttt{pi.001.dat}, etc. (to which a user defined prefix is prepended). Briefly, the information one needs are the likelihood draws, in the second row, and the prior draws in the third row. When \texttt{draw\_from\_prior=0} these will be draws made by comparing the posterior at the accept/reject step of the MCMC chain, as will be true of all other output files such as \texttt{theta.000.dat}, \texttt{theta.001.dat}, etc. When \texttt{draw\_from\_prior=1} these will be draws made by comparing the prior at the accept/reject step of the MCMC chain, as will be true of all other output files. Setting \texttt{draw\_from\_prior=1} when the prior is not proper is a ghastly error.


\textbf{Example:}

\begin{verbatim}
ESTIMATION DESCRIPTION (required)
        test   Project name, project_name, char*
         1.0   bayes_gmm version, version, float
           0   Proposal type, 0 group_move, 1 cond_move, 2 usr, proposaltype, int
           1   Write detailed output if ask_print=1, ask_print, int
  1741133992   Seed for MCMC draws, iseed, int
       10000   Number of MCMC draws per output file, num_mcmc_draws, int
           9   Number of MCMC output files beyond the first, num_mcmc_files, int
        10.0   Rescale prop scale block by this, proposal_scale_factor, float
         1.0   Rescale posterior by this val, temperature, float
           1   Sandwich variance not computed if no_sandwich=1, no_sandwich, int
           0   Number of lags in HAC middle of sandwich variance, lag_hac, int
           5   The thinning parameter used to write MCMC draws, thin, int
           0   Draw from prior if draw_from_prior=1, draw_from_prior, int
\end{verbatim}

\subsection{DATA DESCRIPTION}

In the block labeled DATA DESCRIPTION are parameters that specify the dimension of the data, the number of observations, and govern reading of the data. The data are presumed to be stored in a file containing rows that have values separated by blanks containing the data for each observation $y_t$ and perhaps additional values such as dates or the index $t$. There should be one line for each $t = 1, \dots , n$. The presence of the line terminating character is important because the $C++$ function \texttt{getline} does the reading.

\texttt{M:} The dimension of the vector $y_t$.

\texttt{sample\_size:} The number of observations to be read. The value can be smaller than the number of observations in the file in which case those at the end will not be read.

\texttt{datafilename:} The name of the file from which the data are to be read. The file must be located in the \texttt{data} directory.

\texttt{var\_cols :} Lastly, one has fields. One must use care here because errors can cause the program to crash with misleading diagnostic messages, if any at all. As just mentioned, the presumptions is that the data are arranged in a table with time $t$ as the row index and the elements of $y_t$ in the columns. The blank separated numbers here specify the variables (columns) of the data in the order in which they are to be assigned to the elements $y_{1t}, y_{2t}, \dots, y_{Mt}$ of $y_t$. It does not hurt to have too many fields listed because only the first $M$ are read. The disaster is when there are too few (less than $M$) or one of them is larger than the actual number of columns in the data set. A few of the first and last values of $y_t$ read in are printed in the file \texttt{***.detail.dat} which should be checked to make sure the data were read correctly. \texttt{var\_cols} can be specified as a single digit or as a range. Thus, one can enter either \texttt{1 2 3 5} or \texttt{1:3 5}.


Example:

\begin{verbatim}
DATA DESCRIPTION (required) (model constructor sees realmat data(M,sample_size))
          12   Dimension of the data, M, int
         200   Number of observations, sample_size, int
data.dat       File name, any length, no embedded blanks, datafilename, string
1:12           Read these white space separated var_cols, var_cols, intvec

\end{verbatim}


\subsection{MODEL DESCRIPTION}

The MODEL DESCRIPTION block is straightforward, it gives the dimensions of the parameters of the model.

\texttt{len\_model\_param:} The dimension of $\theta$, which is the parameter vector of the model.

\texttt{len\_model\_func:} The dimension of \texttt{stats}, which is the vector of statistics (functionals) of the model that are computed from a simulation of the model. (\textbf{NOT USED})

Example:

\begin{verbatim}
MODEL DESCRIPTION (required)
          26   Number of model parameters, len_model_param, int
           1   Number of model functionals, len_model_func, int
\end{verbatim}


\subsection{MODEL PARAMFILE}

The vectors \texttt{model\_paramfile\_lines} and \texttt{model\_addlines} of type \texttt{vector<string>} that are passed to the usermodel constructor (in \texttt{main.cpp}) are defined in the MODEL PARAMFILE block.

\texttt{model\_paramfile:} This is the name of a file containing lines of the userâ€™s choosing. This file is read and passed to the usermodel constructor as the \texttt{std$::$vector} of \texttt{std$::$string} \texttt{model\_paramfile\_lines}. If there is no such file then code \texttt{\_\_none\_\_} as the filename.

\texttt{\#begin additional\_lines}, \texttt{\#end additional\_lines:} Lines between these two markers are read and passed to the usermodel constructor as \texttt{model\_addlines} of type \texttt{vector<string>}. The two marker lines are passed as well so that the first user line is \texttt{model\_addlines[1]} and not \texttt{model\_addlines[0]}.

Example:

\begin{verbatim}
MODEL PARAMFILE (required) (goes to usermodel as model_addlines)
__none__       File name, use __none__ if none, model_paramfile, string
#begin additional lines
           2   Number of observable risk factors, numb_obs_factor, int
           1   Lags for observable risk factors, lag_obs_factor, int
           8   Number of log returns, numb_returns, int
           1   Lags for HAC variance estimator (GMM objfun), lag_hac_gmm, int
         500   Number of particles, N, int
         100   Simulation size, len_simul, int
          50   Draws between particle filter updates, particle_update, int
#end additional lines
\end{verbatim}


\subsection{PARAMETER START VALUES}

The block labeled PARAMETER START VALUES specifies the first value for the chain.\footnote{New files \texttt{paramfile.fit}, \texttt{paramfile.end} and \texttt{paramfile.alt} are written as the MCMC chain progress with the current putative mode of the objective function replacing the values in PARAMETER START VALUES for \texttt{.fit} and \texttt{.alt} and the last value of $\theta$ in the chain in the case of \texttt{.end}. The \texttt{paramfile.end} is used to recommence where one left off; \texttt{paramfile.fit} is used to recommence starting at the mode, which is what one usually wants to do; and \texttt{paramfile.alt} is used when switching to the conditional move proposal (\texttt{proposal\_type=1}). If the number of parameters exceeds $20$, then \texttt{paramfile.alt} will not be written. Once the mode has been found, it will not change.} It must satisfy the support conditions; i.e. \texttt{usermodel\_class::support} must return \texttt{true}, and \texttt{usermodel\_class::prior} must return \texttt{scl::dev\_val.positive = true} for this initial value of $\theta$. The numbers to the right, $0$ or $1$, determine whether that element is held fixed or is active. If $0$, then the proposal never moves that element of $\theta$. To the right of this $0$ or $1$ the user may add text such as the name of the parameter.

Example:

\begin{verbatim}
PARAMETER START VALUES (required)
   1.90970554334998099e-01    1         1  A11
   3.24793056338963071e+00    1         2  A21
   3.60281749990822575e-02    1         3  A12
   8.48608238478818777e-02    1         4  A22
\end{verbatim}



\subsection{PROPOSAL SCALING}

These are the standard deviations of the proposal. Ideally, they should be roughly proportional to the standard errors of the estimate of $\theta$ if such is known. Altering their values is a way to affect the rejection probability.

Example:

\begin{verbatim}
PROPOSAL SCALING (required)
   3.12500000000000017e-03              1  A11
   3.12500000000000017e-03              2  A21
   3.12500000000000017e-03              3  A12
   3.12500000000000017e-03              4  A22
\end{verbatim}

\subsection{PROPOSAL GROUPING}


How to specify group moves in the InputParam File is discussed in Subsection $6.3$ of Gallant's EMM User's Guide. Briefly, in each matrix, the first element of the first row gives the relative probability with which this group is selected. In the remaining columns of the first row are the indexes of the parameters in that group. The first column is the same as the first row. The submatrix bounded by the first row and column is a correlation matrix. The multivariate normal to move the group is determined by this correlation matrix and the values in the PROPOSAL SCALING block. Within the PROPOSAL GROUPING block, the index of every parameter must be accounted for. Those parameters that are not moved (i.e. have a $0$ to their right in the PARAMETER START VALUES
block) are collected into a group that is assigned zero probability of being selected. If the PROPOSAL GROUPING block is not present, then one is synthesized. One can view an example (the synthesized version) in the
file \texttt{***.detail.dat}, presuming \texttt{ask\_print=1} in the ESTIMATION DESCRIPTION block.

\pagebreak

\section{How to define your own model}

The most important class that must be written by the user is the \texttt{usermodel\_class}, the main task of which is to evaluate the model's likelihood function. It must be inherited from \texttt{estimator::usermodel\_base}. 

\subsection{The \texttt{usermodel\_class} for gmm based likelihood approximation}

The class is declared in \texttt{usermodel/usermodel.h} and defined in \texttt{usermodel/usermodel.cpp}. Most importantly, it has three key linked private members: these are pointers to (1) a \texttt{moments} class, (2) a \texttt{scl::gmm} class, and (3) a \texttt{model} class\footnote{Defined in this order(!) using the previously defined objects, i.e. \texttt{scl::gmm} is defined using \texttt{moments} and \texttt{model} is defined using \texttt{moments} and \texttt{scl::gmm}.}

\begin{description}

\item[\texttt{moments} class]: evaluates the moment conditions $m(y_t, x_t, \theta)$ (for a specific period t) given \texttt{theta} and the \texttt{data} (and the \texttt{particle} in case of latent variables). The class must be inherited from \texttt{scl::moment\_function\_base} (that has ``compulsory'' methods)

\item[\texttt{scl::gmm} class]: defined in \texttt{base\_model/libscl/}
\begin{itemize}
\item returns $m_n'S_n^{-1} m_n$ where $m_n := \frac{\sum_{t=T_{\min}}^{n} m(y_t, x_t, \theta)}{n-T_{min}+1}$, $T_{\min}\leq n\leq$\texttt{sample\_size} and $S_n$ is 
\begin{itemize}
\item calculated from centered sample moments if \texttt{correct\_for\_mean==true}
\item if \texttt{regularize\_W==true}, it is regularized with parameter \texttt{ridge} (make it well-conditioned)
\item HAC corrected (Parzen weights) if \texttt{lag\_hac\_gmm>0}
\end{itemize}
\item should be initialized with a \texttt{moments} class private member ($T_{\min}$ must be specified as an outcome of the \texttt{moments.get\_minT()} method  
%\item \texttt{logdetW} is the Jacobian of g with respect to theta???
\end{itemize}
\item[\texttt{model} class]: helps to calculate the GMM representation of the likelihood

\begin{itemize}
\item The private members are the ``individual'' model parameters
\item \texttt{likelihood(t)} method calculates the value of the GMM representation evaluated at the data $y_t$ through period $t$, where  $T_{\min}\leq t$ \texttt{sample\_size} (and possibly given the particles). First, it \texttt{set\_particle()} and \texttt{set\_sample\_size}(t) of the \texttt{moments} class, then calls \texttt{scl::gmm} for the current \texttt{theta} value. 
\item Latent variable: two members \texttt{draw\_x0()} and \texttt{draw\_xt(xlag)} and a wrapper \texttt{prop\_yt(t, particle)} that are called by the particle filter 
\end{itemize}


\end{description}

Moreover, corresponding to the particle filter, it has \texttt{scl::realmat} members
\begin{itemize}
\item \texttt{saved\_particle}: used for the conditioning on the last trajectory in the conditioned PF
\item \texttt{draws} (unweighted whole set), \texttt{filter} (reweighted using draws until t), \texttt{smooth} (reweighted using the whole set): $T\times N$ matrices to store the particles from the last updates. They are needed to calculate the mean path and standard deviations
\item \texttt{gibbs\_draws} separately collects the \texttt{theta} mcmc draws for periods when the \texttt{saved\_particle} gets updated 
\end{itemize}

As for the methods, there are three important ones
\begin{enumerate}
\item \texttt{usermodel.support(\texttt{theta})}: boolean variable to determine whether the given \texttt{theta} is inside the support of the prior 
\item \texttt{usermodel.prior(\texttt{theta})}: calculates the log prior density evaluated at a specified $\theta$. The default is a flat prior with $\log p=0$.
\item \texttt{usermodel.likelihood()}: this is the \textbf{particle filter}
\begin{itemize}
\item Before it is called, the \texttt{mcmc.draw()} function sets \texttt{theta\_old} and \texttt{theta} (the proposed $\theta$) 
\item when it is called, the first thing to do is resetting \texttt{theta} for the \texttt{*moments} and \texttt{*model} class members and extract \texttt{sample\_size} and \texttt{T0} from the \texttt{data} and \texttt{*moments} class (\texttt{get\_Tmin}) members respectively
\item set the data and sample size for the \texttt{scl::gmm} class

	  \texttt{bool dset = gmm\_objfun->set\_data(\&data); \\       
      bool nset = gmm\_objfun->set\_sample\_size(n+1); }
\item calculates the likelihood of \texttt{theta}
\begin{enumerate}
\item until \texttt{counter < particle\_update}
\begin{itemize}
\item use \texttt{saved\_particle} and \texttt{t=sample\_size + 1} to call \texttt{*model.likelihood()}
\item increment \texttt{counter} and return \texttt{likelihood(theta\_new)}
\end{itemize}      
\item when \texttt{counter >= particle\_update}, run the particle filter:
\begin{itemize}
\item reset \texttt{theta\_old} for \texttt{*moments}  and \texttt{*models}
\item add \texttt{theta\_old} to \texttt{gibbs\_draws} and initiate \texttt{draws, smooth, filter} with the \texttt{saved\_particle} ($0$th entry of the vector of \texttt{scl::realmat}s)
\item fill the first $T_0=T_{\min}$ elements with \texttt{*model.draw\_x0} and \texttt{*model.draw\_xt}
\item Importance sampling step uses \texttt{*model.prob\_yt} (again \texttt{*model.likelihood}) for the weights (in the background, \texttt{scl::gmm} call with \texttt{theta\_old} and \texttt{data})
\item reset \texttt{saved\_particle} with the last trajectory and return \texttt{likelihood(theta\_new)} (i.e. with the old particles and new theta)
\end{itemize}
\end{enumerate}

\end{itemize}

\end{enumerate} 

\subsection{The \texttt{usermodel\_class} for known (exact) likelihood function}


\pagebreak


\section{Example: the \texttt{sdf} usermodel}

Important parameters to be specified in the MODEL PARAMFILE block:  
\begin{center}
\begin{tabular}{lcl}
Name in InputParam file & Notation & Description\\
\hline
\texttt{numb\_obs\_factor} &  $\mathbf{K}$ & Number of observable macro risk variables \\
\texttt{lag\_obs\_factor} &  $\mathbf{L}$ & Number of lags for the observable macro risk variables \\
\texttt{numb\_returns} & $\mathbf{I}$ & Number of returns used in the estimation
\end{tabular}
\end{center}


The \texttt{data.dat} file, containing $(K + I + 1)$ columns, has the following format
\begin{itemize}
\item \textbf{$1:K$ cols:} Let $Y_t$ be the vector of \textbf{observable} risk factors in a demeaned format, i.e.
\begin{align*}
Y_t := \left[ y_t^1 - \mu^1_y, y_t^2 - \mu^2_y, \dots, y_t^K - \mu^K_y \right]'\hspace{2cm} \mu_y^k := \frac{1}{T}\sum_{t=1}^{T} y^k_t,\quad \forall k\in\{1, \dots, K\}
\end{align*} 
\item \textbf{$K+1$th col:} short risk-free rate proxy $r_t$
\item \textbf{$(K+1):(K+1+I)$ cols:} log risky returns $\log R^i_t$ for various assets $i\in \mathcal{I}$.
\item[] (???) Maybe after that could come the conditioning variables
\end{itemize}

The statistical model is the following
\begin{align*}
\begin{bmatrix} 
Y_{t+1} \\ X_{t+1}
\end{bmatrix} &=  
\begin{bmatrix} 
\textcolor{orange}{A_y} & 0 \\ 0 & \textcolor{blue}{A_x}
\end{bmatrix}\begin{bmatrix} 
Y_{t} \\ X_{t}
\end{bmatrix} + 
\begin{bmatrix} 
\textcolor{orange}{C_y} & 0\\ 0 & \textcolor{blue}{C_x}
\end{bmatrix}\varepsilon_{t+1} \\
\lambda_t &= \textcolor{blue}{\lambda_0} + \begin{bmatrix} 
\textcolor{blue}{\lambda_y} & \textcolor{blue}{\lambda_x} 
\end{bmatrix}\begin{bmatrix} 
Y_{t} \\ X_{t}
\end{bmatrix} \quad\quad\quad \text{with}\quad \lambda_t \in \mathbb{R}^{\text{dim}(\varepsilon)}\\
\log\left( \frac{S_{t+1}}{S_t}\right) &= - r_t - \frac{|\lambda_t|^2}{2}  - \lambda_t \cdot  
\varepsilon_{t+1} 
\end{align*} 
implying the following moment conditions
\begin{align*}
\mathbf{0}_{K\times KL} &= \mathbf{m}_{1}(y_{t+1}, y_t, \theta) = E\left[\left(Y_{t+1} - \sum_{l=1}^{L}\textcolor{orange}{A_{y, l}} Y_{t+1-l}\right)\begin{bmatrix}
Y'_t & Y'_{t-1} & \dots & Y'_{t-L}
\end{bmatrix}\right] \\
\mathbf{0}_{K\times K} &= \mathbf{m}_{2}(y_{t+1}, y_t, \theta) = E\left[\left(Y_{t+1} - \sum_{l=1}^{L}\textcolor{orange}{A_{y, l}} Y_{t+1-l}\right)\left(Y_{t+1} - \sum_{l=1}^{L}\textcolor{orange}{A_{y, l}} Y_{t+1-l}\right)'\right] - \textcolor{orange}{C_yC_y'}\\
0 &= \mathbf{m}_{3}(x_{t+1}, x_t, \theta) = E\left[\left(X_{t+1} - \textcolor{blue}{A_{x}} X_{t}\right)X_t\right] \\
0 &= \mathbf{m}_{4}(x_{t+1}, x_t, \theta) = E\left[\left(X_{t+1} - \textcolor{blue}{A_{x}} X_{t}\right)^2\right] - \textcolor{blue}{C_x}^2\\
\end{align*}
and -- where we define $\textcolor{blue}{\Lambda} := \begin{bmatrix} 
\textcolor{blue}{\lambda_0} & \textcolor{blue}{\lambda_y} & \textcolor{blue}{\lambda_x} 
\end{bmatrix}$ --
\begin{align*}
0 &= \mathbf{m}^i_{5}(z_{t+1}, z_t, x_{t+1}, x_t, \theta) = E\left[\exp\left(-r_t - \frac{1}{2}\begin{bmatrix}
1 \\ Y_t \\ X_t
\end{bmatrix}'\textcolor{blue}{\Lambda'\Lambda}\begin{bmatrix}
1 \\ Y_t \\ X_t
\end{bmatrix} \right.\right.\\ 
&\left.\left.\hspace{2cm}- \left(\textcolor{blue}{\Lambda}\begin{bmatrix}
1 \\ Y_t \\ X_t
\end{bmatrix}\right)'\begin{bmatrix} 
\textcolor{orange}{C_y} & 0\\ 0 & \textcolor{blue}{C_x}
\end{bmatrix}^{-1}\begin{bmatrix}
\left(Y_{t+1} - \sum_{l=1}^{L}\textcolor{orange}{A_{y, l}} Y_{t+1-l}\right)\\
X_{t+1}-\textcolor{blue}{A_x} X_t
\end{bmatrix} + \log R^i_{t+1} \right) - 1 \right]
\end{align*} 

The $\theta$ vector is of length $K^2L + \frac{K(K+1)}{2} + 2 + (K+1)(K+2)$ with 
\begin{align*}
\theta :=\begin{bmatrix}
 \text{vec}\left(A_{y, 1}\right)' & \dots & \text{vec}\left(A_{y, L}\right)' & \text{vec}\left(C_y\right) & \rho & \sigma & \text{vec}\left(\Lambda\right)'
\end{bmatrix}
\end{align*}

\subsection{Structure of \texttt{data.dat}}

The program presumes that the data file has the following format


\begin{table}[!htb]
\begin{minipage}{.7\linewidth}
\centering
\begin{tabular}{cccccccc}
& $Y_{1, -L+1}$ & \dots & $Y_{K, -L+1}$ & NaN & NaN & \dots & NaN \\
& \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
& $Y_{1, 0}$ & \dots & $Y_{K, 0}$ & $r_0$ & NaN & \dots & NaN \\ \hline
\textcolor{mygray}{$m_1, \varepsilon^Y_1 \ \leftarrow$} & $Y_{1, 1}$ & \dots & $Y_{K, 1}$ & $r_1$ & $\log R^{1}_1$ & \dots & $\log R^{I}_1$ \\ 
& \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
\textcolor{mygray}{$m_t, \varepsilon^Y_t \ \leftarrow$} & $Y_{1, t}$ & \dots & $Y_{K, t}$ & $r_t$ & $\log R^{1}_t$ & \dots & $\log R^{I}_t$ \\ 
& \dots & \dots & \dots & \dots & \dots & \dots & \dots \\
\textcolor{mygray}{$m_T, \varepsilon^Y_T \ \leftarrow$} & $Y_{1, T}$ & \dots & $Y_{K, T}$ & NaN & $\log R^{1}_T$ & \dots & $\log R^{I}_T$ \\ 
\end{tabular}
\caption*{\texttt{data.dat}}
\end{minipage}%
\begin{minipage}{.3\linewidth}
\centering
\begin{tabular}{cc}
 NaN  & \\
\dots & \\
$X_0$ & \\\hline
$X_1$ & \textcolor{mygray}{$\rightarrow \ \varepsilon^X_1$} \\
\dots & \\
$X_t$ & \textcolor{mygray}{$\rightarrow \ \varepsilon^X_t$} \\
\dots & \\
$X_T$ & \textcolor{mygray}{$\rightarrow \ \varepsilon^X_T$} \\      
\end{tabular}                
\caption*{\texttt{initial\_particle.dat}}
\end{minipage} 
\end{table}
where the NaN's denote entries that are \textbf{not used} during the estimation so the associated values are inconsequencial. The gray entries denote the shocks $\varepsilon_t$ calculated by using the $t:(t+L)$ row-block.   

$T$ is the effective sample size (number of terms in $m_T$ defined below), so that the files \texttt{data.dat} and \texttt{initial\_particle.dat} (optional) have $T+L$ rows\footnote{This is equal to the initial.\texttt{len\_history} (private) variable of the gmm and moment classes.} The \texttt{sample\_size} variable in the InputParam File must be equal to $T+L$.

\pagebreak 

\subsection{Sampler}

\noindent Let $Z_t \equiv [Y'_{t}, Y'_{t-1}, \dots, Y'_{t-L}, r_t, \log R_{t}^1, \dots, \log R_{t}^I]'$ (see the structure of the \texttt{data.dat} file in the next subsection). Our aim is to sample from the joint posterior density 
\begin{align*}
p(x^T, \theta | z^T) \propto \underbrace{p^0(z^T | x^T, \theta)}_{\text{measurement}}\underbrace{p^0(x^T|\theta)}_{\text{state transition}}\underbrace{p^0(\theta)}_{\text{prior}}
\end{align*}
We replace the measurement equation with the ``GMM representation''
\begin{align*}
p^0(z^T | x^T, \theta) \approx (2\pi)^{\frac{M}{2}}\exp\left(-\frac{T}{2} m_T(z^T; \theta, x^T)'[W(z^T;\theta, x^T)]^{-1}m_T(z^T; \theta, x^T)\right)
\end{align*}
with, let $\tilde{x_t}:=[x_t, x_{t-1}]'$, 
\begin{align*}
m_T(z^T; \theta, x^T) &:=  \frac{1}{T}\sum_{t=1}^T m(z_t, \tilde{x}_t, \theta) 
\end{align*}
and the HAC weighting matrix (with Parzen weights) is
\begin{align*}
W(z^T;\theta, x^T) &:= \sum_{\tau = - \lfloor T^{\frac{1}{5}}\rfloor}^{\lfloor T^{\frac{1}{5}}\rfloor} \widehat w\left(\frac{\tau}{\lfloor T^{\frac{1}{5}}\rfloor}\right)W_{\tau}(z^T; \theta, x^T) \\
\widehat W_{\tau}(z^T; \theta, x^T) &:= \left\{ \begin{array}{ll}
\frac{1}{T}\sum_{t=1+\tau}\left[m(z_t, \tilde{x}_t, \theta) - m_T(z^T; \theta, x^T)\right]\left[m(z_{t-\tau}, \tilde{x}_{t-\tau}, \theta) - m_T(z^T; \theta, x^T)\right]' & \tau\geq 0 \\
\widehat W'_{-\tau}(z^T; \theta, x^T) & \tau<0
\end{array}\right. \\
w(h) &:= \left\{ \begin{array}{ll}
1 - 6|u|^2 + 6|u|^3 & 0 \leq u < \frac{1}{2} \\
2(1-|u|)^3 & \frac{1}{2}\leq u \leq 1
\end{array}\right. 
\end{align*}
$\lfloor x \rfloor$ denotes the integer nearest of $x$. In theory, $\lfloor T^{\frac{1}{5}} \rfloor$ should be equal to \texttt{lag\_hac\_gmm} in the InputParam File. If \texttt{lag\_hac\_gmm}$=0$, we implicitly \textbf{assume} that the moment conditions are serially uncorrleated and so heteroscedastic autoregressive adjustment is not needed. 

Because the GMM representation of the measurement density can only be evaluated point-wise, we need to use Monte Carlo. In particular, we use the so called \textbf{Particle Gibbs Sampler}: In an idealized setup, the posterior can be targeted by iterationg on the following two conditional steps
\begin{enumerate}
\item Draw $\tilde x^T$ from $x^T | \theta, z^T$\hspace{9mm}$\Rightarrow$\hspace{5mm} conditional particle filter of \cite{ADH_2010} 
\item Draw $\tilde{\theta}$ from $\theta | \tilde x^T, z^T$ \hspace{1cm}$\Rightarrow$\hspace{5mm} Metropolis-Hastings using \cite{ChernozhukovHong_2003}
\end{enumerate}\vspace{3mm}
More precisely, the algorithm is the following
\begin{algorithm}[htb]
 \SetKw{Init}{1. Initialization}
 \SetKw{Iter}{2. Iteration $t>T_0$}


 \SetStartEndCondition{ }{}{}%
 \SetKwProg{Fn}{def}{\string:}{}
 \SetKwFunction{Range}{range}%%
 \SetKwFunction{Metropolis}{MetropolisStep}%%
 \SetKwFunction{PF}{CondParticleFilter}%%
 \SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
 \SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
 \SetKwFor{While}{while}{:}{fintq}%
 \SetInd{.5em}{2em} 
 %\renewcommand{\forcond}{$i$ \KwTo\Range{$n$}}
 %\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%
   
 \KwIn{$\theta^{(0)}$, $x^{(0)}_{1:T}$ and $z_{1:T}$}
 \BlankLine
 \For{$j$ \KwTo \Range{$R$}}{
 Draw $x_{1:T}^{(j+1)}$ from \PF{$\theta^{(j)}$, $x_{1:T}^{(j)}$, $z_{1:T}$}\\
 \BlankLine
 Draw $\theta^{(j+1)}$ from \Metropolis{$\theta^{(j)}$, $x_{1:T}^{(j+1)}$, $z_{1:T}$}\\
 }
 \BlankLine  
 \KwOut{MCMC chain $\{\theta^{(j)}, x_{1:T}^{(j)}\}_{j=1}^R$}
 \caption{Particle Gibbs Sampler}
\end{algorithm}

With the two components


\begin{algorithm}[H]
 \SetKw{Init}{1. Initialization}
 \SetKw{Iter}{2. Iteration $t>T_0$}


 \SetStartEndCondition{ }{}{}%
 \SetKwProg{Fn}{def}{\string:}{}
 \SetKwFunction{Range}{range}%%
 \SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
 \SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
 \SetKwFor{While}{while}{:}{fintq}%
 \SetInd{.5em}{2em} 
 %\renewcommand{\forcond}{$i$ \KwTo\Range{$n$}}
 %\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%
   
 \KwIn{$\theta^{(j)}$, $x^{(j)}_{1:T}$ and $z_{1:T}$}
 \BlankLine
 Set the first particle $x^1_{1:T} = x^{(j)}_{1:T}$ 
 \BlankLine
 Set $T_0$ for the training sample\; 
 \BlankLine
  \For{$i$ \KwTo\Range{$\mathbf{2}$, $N$}}{
  sample the particle path $\tilde{x}_{1:T_0}^i$ from $p^0\left(x_t | x_{t-1}, \theta\right)$ \\
 \BlankLine  
  set $x^i_{1:T_0} = \tilde{x}_{1:T_0}^i$
  }
  \BlankLine
 \For{$t$ \KwTo\Range{$T_0+1$, $T$}}{
	compute $w^1_t\left(\theta\right) = p\left(z_{1:t} \ | \ x^1_{1:t}, \theta\right)$ 
  
  \tcc{Importance sampling}  
  \For{$i$ \KwTo\Range{$N$}}{
  sample $\tilde{x}^i_t$ from $p^0\left(x_t | x^{i}_{t-1}, \theta\right)$\\
 \BlankLine  
  set $\tilde{x}^i_{1:t} = (x^i_{1:t-1}, \ \tilde{x}^i_t)$\\
  \BlankLine    
  compute $w^i_t\left(\theta\right) = p\left(z_{1:t} \ | \ \tilde{x}^i_{1:t}, \theta\right)$\\
  
  }
 \BlankLine  
  scale the weigths $W_t^i(\theta) = \frac{w^i_t\left(\theta\right)}{\sum_{i=1}^N w^i_t\left(\theta\right)}$\;
 \BlankLine  
  \tcc{Resampling}  
  sample $N-1$ paths $\{x^i_{1:t}\}_{i=\mathbf{2}}^N$ from the set $\{\tilde{x}^i_{1:t}\}_{i=1}^{N}$ with prob $\{W^i_t(\theta)\}_{i=1}^{N}$
  }
 \BlankLine  
 \KwOut{Updated path $x^{(j+1)}_{1:T} = x^N_{1:T}$}

 \caption{Conditional Partical Filter}
\end{algorithm}
\vspace{8mm}

\begin{algorithm}[H]
 \SetKw{Init}{1. Initialization}
 \SetKw{Iter}{2. Iteration $t>T_0$}


 \SetStartEndCondition{ }{}{}%
 \SetKwProg{Fn}{def}{\string:}{}
 \SetKwFunction{Range}{range}%%
 \SetKw{KwTo}{in}\SetKwFor{For}{for}{\string:}{}%
 \SetKwIF{If}{ElseIf}{Else}{if}{:}{elif}{else:}{}%
 \SetKwFor{While}{while}{:}{fintq}%
 \SetInd{.5em}{2em} 
 %\renewcommand{\forcond}{$i$ \KwTo\Range{$n$}}
 %\AlgoDontDisplayBlockMarkers\SetAlgoNoEnd\SetAlgoNoLine%
   
 \KwIn{$\theta^{(j)}$, $x^{(j+1)}_{1:T}$ and $z_{1:T}$}
 \BlankLine
 \tcc{Proposal}  
 Draw $\theta^{\text{prop}}$ from $q\left(\theta | \theta^{(j)}\right)$
 \BlankLine
 Set $\alpha = \min\left\{1, \ \frac{p\left(z_{1:T}| x^{(j+1)}_{1:T}, \theta^{\text{prop}}\right)}{p\left(z_{1:T} | x^{(j+1)}_{1:T}, \theta^{(j)}\right)} \right\}$ 
 \BlankLine
 \tcc{Accept-Reject}  
 Draw $u\sim U[0,1]$\\
 \BlankLine
 
 \uIf{$u<\alpha$}{
	  $\theta^{(j+1)} = \theta^{\text{prop}}$	
 }
 \Else{
	  $\theta^{(j+1)} = \theta^{(j)}$ 	
 }

 \BlankLine  
 \KwOut{Updated $\theta^{(j+1)}$}

 \caption{Metropolis-Hastings step}
\end{algorithm}

\subsection*{Recursive formulas for the mean and autocovariance estimators}

To speed up the particle filter, we ...

Evidently, the mean can be written as
\begin{align*}
\mu_{T} := T m_T = \sum_{t=1}^{T} m_t = \mu_{T-1} + m_T \quad \Rightarrow \quad m_T = \frac{\mu_{T-1} + m_T}{T}
\end{align*}

The estimator for the autocovariance of length $\tau$ is
\begin{align*}
\widehat\gamma_T(\tau) = \frac{1}{T} \sum_{t=1+\tau}^{T}(x_t - \mu)(x_{t-\tau}-\mu)'
\end{align*}
This is biased (should devide by $T-\tau$), but its variance is smaller than that of the unbiased.
\begin{align*}
T\widehat\gamma_T(\tau) &= \sum_{t=1+\tau}^{T}x_t x'_{t-\tau} - \sum_{t=1+\tau}^{T} (x_t\mu' + \mu x'_{t-\tau}) + \sum_{t=1+\tau}^{T}\mu\mu' = \\
&= \sum_{t=1+\tau}^{T}x_t x'_{t-\tau} - 2T\mu\mu' + (T-\tau)\mu\mu' + (x_1 + \dots + x_{1+\tau-1})\mu' + \mu(x_{T-\tau + 1} + \dots + x_T)' \\
&= \underbrace{\sum_{t=1+\tau}^{T}x_t x'_{t-\tau}}_{:= R_T(\tau)} - (T+\tau)\mu\mu' + (x_1 + \dots + x_{1+\tau-1})\mu' + \mu(x_{T-\tau + 1} + \dots + x_T)' 
\end{align*}
The covariance matrix follows from this formula with $\tau=0$
\begin{align*}
 R_T(0) &= R_{T-1}(0) - T \mu_T\mu_T' \quad \Rightarrow \quad \widehat{\gamma}_T(0)= \frac{R_{T-1}(0)}{T} - \mu_T\mu_T'
\end{align*}
Using these recursions we can compute $\mu_T, R_T(0), R_T(\tau)$ from $\mu_{T-1}, R_{T-1}(0), R_{T-1}(\tau)$.


\pagebreak


\section*{The \texttt{base\_model/initialize/main.cpp} file }

\begin{enumerate}
\item The \texttt{main} is called with command arguments with possibly multiple rows, e.g. the file \texttt{control.dat} might contain 
\begin{verbatim}
svsim.param.000 svsim_0
svsim.param.001 svsim_1 
\end{verbatim} 
and the program reads rows iteratively, at each round \texttt{argp[1]} becomes \texttt{paramfile}, \texttt{prefix}

\item Create \textbf{specification class} (called \texttt{specification}) and take the rows of \texttt{paramfile} to pass them into \texttt{specification.set\_params()}. This initializes the private members of \texttt{specification\_class}
\begin{itemize}
\item[-] the \texttt{(estblock, datablock, modelblock)} triple using the first three blocks of the InputParamFile (see above)
\item[-] set the value of \texttt{theta} (and \texttt{theta\_fixed}) using PARAMETER START VALUES block
\item[-] set \texttt{proposal\_scale} as a product of \texttt{estblock.proposal\_scale\_factor} and the PROPOSAL SCALING block
\item[-] set \texttt{proposal\_groups} from the PROPOSAL GROUP block if provided
\end{itemize} 
\item Read data from the \texttt{data/} folder using \texttt{datablock.read\_data()}
\begin{itemize}
\item[-] data file must contain columns for observables
\item[-] columns must be separated by whitespace
\end{itemize}
\item Create \textbf{usermodel class} (of \texttt{usermodel\_type}) named as \texttt{usermodel} with the arguments
\begin{itemize}
\item just imported \texttt{data}
\item \texttt{model\_blk.len\_model\_param} from the InputParamFile
\item \texttt{model\_blk.len\_model\_func} from the InputParamFile
\item \texttt{specification.get\_model\_addlines()} from the InputParamFile 
\end{itemize}
 
\begin{center}
\line(1,0){450}
\end{center}

What does this do? (see model/usermodel/usermodel.cpp)
\begin{enumerate}
\item[(a)] read \texttt{initial\_particle.dat} to set the value of \texttt{saved\_particle} member of \texttt{usermodel}
\item[(b)] initialize \texttt{moment\_cond}$=$\texttt{moments()}
          \begin{itemize}
          \item default: \texttt{data(0), particle(0), n(0), lag\_gmm(0), theta(4, 1)}
          \item \texttt{theta} gets initialized from \texttt{default\_params.h} default values
		  \end{itemize}
\item[(c)] initialize \texttt{gmm\_objfun}$=$\texttt{gmm(moment\_cond, lag\_gmm, \&data, data.ncol(), lag\_hac\_gmm)} with regularized $W$ (ridge$=1.0e-3$)
          \begin{itemize}
          \item default: \texttt{mf(moment\_cond), mfl(lag\_gmm), data, sample\_size(data.ncol()), \\ Lhac(lag\_hac\_gmm), correct\_for\_mean(true), regularize\_W(false), ridge(0.0), warning\_messages(true)}          
          \item reset \texttt{lag\_gmm, data, n} for \texttt{moment\_cond} from these (!!!)
          \item run \texttt{gmm\_objfun->set\_regularize\_W(true, 1.0e-3)}
		  \end{itemize}
\item[(d)] initialize \texttt{model}$=$\texttt{model(moment\_cond, gmm\_objfun)}
          \begin{itemize}
          \item default: parameters (\texttt{default\_params.h}), \texttt{moment\_cond} and \texttt{gmm\_objfun}		          
          \item run \texttt{gmm\_objfun->set\_moment\_function(moment\_cond)}, i.e. 
          \begin{itemize}
          \item set\_lag\_gmm(), set\_data() and set\_sample\_size() for \texttt{moment\_cond} from provided objects (!!!) (this is insurance, should be redundant)
          \end{itemize}
              
          \end{itemize}
\item[(e)] \texttt{lag\_gmm, lag\_hac\_gmm, N, len\_simul, particle\_update} get read from\\ \texttt{specification.get\_model\_addlines()}
\item[(f)] instantiate \texttt{draws, smooth} and \texttt{filter} of \texttt{usermodel} with length \texttt{N}
\end{enumerate}

\begin{center}
\line(1,0){450}
\end{center}

\item Create \textbf{proposal class} (of \texttt{proposal\_base} type) named \texttt{proposal} using 
\begin{itemize}
\item[-] \texttt{estblock.proposaltype} (\texttt{group\_move} or \texttt{conditional\_move}) and
\item[-] \texttt{specification.get\_proposal\_groups()} defined above
\end{itemize}
 
 \item Create the \textbf{mcmc class} using the \texttt{proposal} and \texttt{usermodel} classes
 \begin{itemize}
 \item default: \texttt{proposal(proposal), usermodel(usermodel), simulation\_size(1), thin(1), \\
               draw\_from\_posterior(true), temperature(1.0), posterior\_mode(), \\ posterior\_maxval(-REAL\_MAX)}
 \item reset elements from the ESTIMATION DESCRIPTION block:
\begin{itemize}
\item \texttt{simulation\_size} becomes \texttt{len\_mcmc\_draws}
\item reset \texttt{thin, draw\_from\_posterior, temperature} from \texttt{est\_blk}
\end{itemize}        
\end{itemize}
\item Create \textbf{asymptotics class} (of \texttt{asymptotics\_base} type) with args \texttt{data, usermodel, mcmc}
\begin{itemize}
\item default: \texttt{data(data), T(data.get\_cols()), mcmc(mcmc), \\
len\_theta(usermodel.get\_len\_theta()), theta\_sum(len\_theta, 1, 0.0), \\ mean\_old(len\_theta, 1, 0.0), theta\_sse(len\_theta, len\_theta, 0.0),\\
              mean(len\_theta, 1, 0.0), posterior\_mode(len\_theta, 1, 0.0), \\ cov(len\_theta, len\_theta, 0.0),
              foc(len\_theta, 1, 0.0), cum\_sample\_size(0), \\ posterior\_maxval(-REAL\_MAX)}
\end{itemize}

\item  Define
\texttt{realmat theta = specification.get\_theta();} and \texttt{INT\_32BIT seed = est\_blk.seed;}

\item Loop \texttt{num\_mcmc\_files} many times over the sampler of size \texttt{num\_mcmc\_draws} starting at \texttt{theta}
\begin{description}
\item[\textbf{[A]}] \texttt{mcmc.draw(seed, theta\_start, theta\_sim, stats\_sim, pi\_sim);}
\begin{itemize}
\item this function communicates with PF in \texttt{usermodel.likelihood()}
\begin{itemize}
\item upon new proposal mcmc class tells usermodel the new and old theta's
\item it draws particle\_update many proposals before a new set of particles
\item it returns the rejection probabilities as a table: rejection/total
\end{itemize}
\item \texttt{realmat theta\_old = theta\_start;} \\

    \texttt{usermodel.set\_theta(theta\_start); \\
    usermodel.set\_theta\_old(theta\_old); \\
    usermodel.get\_stats(stats\_old);} \\
    
    \texttt{den\_val likehood\_old = \textcolor{red}{usermodel.likelihood()};  \\  
    den\_val prior\_old = usermodel.prior(theta\_old, stats\_old); \\  
    den\_val pi\_old = prior\_old; \\
    if (draw\_from\_posterior) { pi\_old += likehood\_old; } \\
    posterior\_mode = theta\_start;  \\
    posterior\_maxval = pi\_old.log\_den; \\             
    if (pi\_old.positive) pi\_old.log\_den *= temperature;   } 
\item Define \texttt{*\_new} variables from \texttt{*\_old} ones      
\item \texttt{num\_mcmc\_draws} many times
\begin{itemize}
\item \texttt{proposal.draw(jseed, theta\_old, theta\_new); \\
		usermodel.set\_theta(theta\_new); \\
        usermodel.set\_theta\_old(theta\_old);}
\item  \texttt{if (usermodel.support(theta\_new) \&\& usermodel.get\_stats(stats\_new))}
\begin{itemize}
\item call \textcolor{red}{\texttt{usermodel.likelihood()}}
\item update \texttt{likelihood\_new, prior\_new, pi\_new}
\item if \texttt{pi\_new > high}, reset \texttt{posterior\_mode} and \texttt{posterior\_maxval}
\item calculate alpha from \texttt{pi\_old} and \texttt{pi\_new} and rejection/acceptance step
\end{itemize}
\end{itemize}
\item last simulated value becomes \texttt{theta\_start}, so we can use that again in the \texttt{num\_mcmc\_files} many iterations
\end{itemize}

\item[\textbf{[B]}] \texttt{asymptotics.set\_asymptotics(theta\_sim);}
\begin{itemize}
\item  taking \texttt{theta\_sum}, this updates asymptotics' \texttt{mean}, \texttt{posterior\_mode, cov, cum\_sample\_size} and \texttt{posterior\_maxval}

\end{itemize}
\item[\textbf{[C]}] \texttt{asymptotics.get\_asymptotics(theta\_hat, V\_hat, T);} 
\begin{itemize}
\item[-] assigns \texttt{mean, cov/T, T} to arguments
\end{itemize}
\item[\textbf{[C]}] \texttt{asymptotics.get\_asymptotics(theta\_mean, theta\_mode, posterior\_high, I, invJ,\\ foc\_hat, reps);} 
\begin{itemize}
\item[-] assigns \texttt{mean, posterior\_mode, posterior\_maxval, I = null; invJ = cov; foc\_hat = null; reps = 0;} to arguments
\end{itemize}


\item[\textbf{[D]}] \texttt{usermodel.set\_theta(theta\_mode)}: reset usermodel's \texttt{theta} with the new mode
\item[\textbf{[E]}] \texttt{specification.write\_params(paramfile, prefix, seed, theta, theta\_mode, invJ/T)\\
usermodel.set\_theta(theta\_mode); \\
filename = "../result\_files/" + prefix + ".usrvar"; \\
usermodel.write\_usrvar(filename.c\_str());}
\item[\textbf{[F]}] call \texttt{output} function
\begin{itemize}
\item  \texttt{theta\_mean ->  prefix + ".theta\_mean.dat";}
\item  \texttt{theta\_mode -> prefix + ".theta\_mode.dat";}
\item  \texttt{V\_hat\_hess ->  prefix + ".V\_hat\_hess.dat";}
\item  \texttt{theta\_mean, theta\_mode, V\_hat\_hess -> prefix + ".summary.dat";}
\item  \texttt{theta\_sim -> prefix + ".theta." + number + ".dat";}
\item  \texttt{stats\_sim -> prefix + ".stats." + number + ".dat";}
\item  \texttt{pi\_sim -> prefix + ".pi." + number + ".dat";}
\item  \texttt{reject -> prefix + ".reject." + number + ".dat";}
\end{itemize}
\end{description}

\end{enumerate}



\pagebreak


\section*{Notes}

\begin{itemize}
\item Usermodel class have theta and theta\_old member. During the mcmc sampling, thest private members get updated. Calling usermodel.likelihood(), usermodel automatically updates *\_model and *\_moments classes as well
\end{itemize}

Both mcmc class and asymptotics class have posterior\_mode and posterior\_maxval members
\begin{itemize}
\item mcmc versions get updated just before the acceptance/rejection step 
\item temperature for posterior affects the acceptance/rejection step 
\item after \texttt{num\_mcmc\_draws} many draws the mcmc versions are passed to asymptotics $=>$ \texttt{theta\_mode} and \texttt{posterior\_high} get updated in asymptotics along with the cumulative mean theta and cov $=>$ everything gets written into summary.dat files 
\end{itemize}


\newpage
\appendix
\vspace{48pt}
%====================================0000====================================
\bibliographystyle{plainnat}
\bibliography{reference}
%====================================0000====================================
\end{document}

